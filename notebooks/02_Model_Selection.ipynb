{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CUNEF MUCD 2021/2022\n",
    "## News Classification\n",
    "Autor:  \n",
    "- Antonio Tello GÃ³mez"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Model Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Basics\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pickle\n",
    "\n",
    "\n",
    "#Functionalities\n",
    "from collections import Counter\n",
    "import sys, os\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "#NLP\n",
    "import string\n",
    "import re\n",
    "import nltk\n",
    "\n",
    "\n",
    "# Machine Learning\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer \n",
    "from sklearn.pipeline import Pipeline \n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "#Metrics\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "# Custom Transformer\n",
    "sys.path.append(os.path.abspath('..'))\n",
    "sys.path.append(os.path.abspath('../src'))\n",
    "from src.Preprocessor import TextPreprocessor\n",
    "\n",
    "# Models\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.dummy import DummyClassifier\n",
    "from lightgbm import LGBMClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.linear_model import Perceptron\n",
    "from sklearn.linear_model import PassiveAggressiveClassifier\n",
    "from sklearn.linear_model import SGDClassifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load the clean data \n",
    "#(However, we will use the original data to test the custom transformer)    \n",
    "df = pd.read_csv('../data/fake_or_real_news_clean.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train Test Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train-test split\n",
    "xtrain, xtest, ytrain, ytest = train_test_split(df['full_text'], df['label'], test_size=0.25, random_state=2022, stratify=df['label'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save train in train folder\n",
    "xtrain.to_csv('../data/train/X_train.csv', index=False)\n",
    "ytrain.to_csv('../data/train/y_train.csv', index=False, header=['label'])\n",
    "\n",
    "# Save test in test folder \n",
    "xtest.to_csv('../data/test/X_test.csv', index=False)\n",
    "ytest.to_csv('../data/test/y_test.csv', index=False, header=['label'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Base Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The base model is a dummy model that always predicts the same value. i.e. all news are real.  \n",
    "It is used to compare the performance of the other models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_model = Pipeline([\n",
    "    (\"preprocessor\", TextPreprocessor()),\n",
    "    (\"vectorizer\", TfidfVectorizer()),\n",
    "    (\"clf\", DummyClassifier(strategy=\"most_frequent\"))\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(steps=[('preprocessor', TextPreprocessor()),\n",
       "                ('vectorizer', TfidfVectorizer()),\n",
       "                ('clf', DummyClassifier(strategy='most_frequent'))])"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "base_model.fit(xtrain, ytrain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5003170577045022"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "base_model.score(xtest, ytest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "ypred = base_model.predict(xtest)\n",
    "ypred_proba = base_model.predict_proba(xtest)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Function to evaluate the model using popular metrics:  \n",
    "ROC-AUC,Accuracy,Precision,Recall,F1-score,Confusion matrix..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(ytest, ypred, ypred_proba = None):\n",
    "    if ypred_proba is not None:\n",
    "        print('ROC-AUC score of the model: {}'.format(roc_auc_score(ytest, ypred_proba[:, 1])))\n",
    "    print('Accuracy of the model: {}\\n'.format(accuracy_score(ytest, ypred)))\n",
    "    print('Classification report: \\n{}\\n'.format(classification_report(ytest, ypred)))\n",
    "    print('Confusion matrix: \\n{}\\n'.format(confusion_matrix(ytest, ypred)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ROC-AUC score of the model: 0.5\n",
      "Accuracy of the model: 0.5003170577045022\n",
      "\n",
      "Classification report: \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.50      1.00      0.67       789\n",
      "           1       0.00      0.00      0.00       788\n",
      "\n",
      "    accuracy                           0.50      1577\n",
      "   macro avg       0.25      0.50      0.33      1577\n",
      "weighted avg       0.25      0.50      0.33      1577\n",
      "\n",
      "\n",
      "Confusion matrix: \n",
      "[[789   0]\n",
      " [788   0]]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "evaluate_model(ytest, ypred, ypred_proba)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The accuracy of the base model is 0.5\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training and Model Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have a baseline we can start to compare the performance of the other models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are going to use the following models:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifiers =[\n",
    "    LogisticRegression(),\n",
    "    MultinomialNB(),\n",
    "    Perceptron(),\n",
    "    PassiveAggressiveClassifier(),\n",
    "    SGDClassifier(),\n",
    "    RandomForestClassifier(),\n",
    "    XGBClassifier(),\n",
    "    LGBMClassifier()\n",
    "    ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This loop fits the models in the train data and then generates a report with the metrics in the test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Saved\n",
      "LogisticRegression()\n",
      "Accuracy of the model: 0.9226379201014585\n",
      "\n",
      "Classification report: \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.95      0.89      0.92       789\n",
      "           1       0.90      0.95      0.92       788\n",
      "\n",
      "    accuracy                           0.92      1577\n",
      "   macro avg       0.92      0.92      0.92      1577\n",
      "weighted avg       0.92      0.92      0.92      1577\n",
      "\n",
      "\n",
      "Confusion matrix: \n",
      "[[705  84]\n",
      " [ 38 750]]\n",
      "\n",
      "------------------------------------------------------\n",
      "Model Saved\n",
      "MultinomialNB()\n",
      "Accuracy of the model: 0.8344958782498415\n",
      "\n",
      "Classification report: \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.76      0.98      0.86       789\n",
      "           1       0.97      0.69      0.81       788\n",
      "\n",
      "    accuracy                           0.83      1577\n",
      "   macro avg       0.87      0.83      0.83      1577\n",
      "weighted avg       0.87      0.83      0.83      1577\n",
      "\n",
      "\n",
      "Confusion matrix: \n",
      "[[775  14]\n",
      " [247 541]]\n",
      "\n",
      "------------------------------------------------------\n",
      "Model Saved\n",
      "Perceptron()\n",
      "Accuracy of the model: 0.9226379201014585\n",
      "\n",
      "Classification report: \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.90      0.95      0.92       789\n",
      "           1       0.94      0.90      0.92       788\n",
      "\n",
      "    accuracy                           0.92      1577\n",
      "   macro avg       0.92      0.92      0.92      1577\n",
      "weighted avg       0.92      0.92      0.92      1577\n",
      "\n",
      "\n",
      "Confusion matrix: \n",
      "[[747  42]\n",
      " [ 80 708]]\n",
      "\n",
      "------------------------------------------------------\n",
      "Model Saved\n",
      "PassiveAggressiveClassifier()\n",
      "Accuracy of the model: 0.9365884590995561\n",
      "\n",
      "Classification report: \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.94      0.93      0.94       789\n",
      "           1       0.93      0.94      0.94       788\n",
      "\n",
      "    accuracy                           0.94      1577\n",
      "   macro avg       0.94      0.94      0.94      1577\n",
      "weighted avg       0.94      0.94      0.94      1577\n",
      "\n",
      "\n",
      "Confusion matrix: \n",
      "[[735  54]\n",
      " [ 46 742]]\n",
      "\n",
      "------------------------------------------------------\n",
      "Model Saved\n",
      "SGDClassifier()\n",
      "Accuracy of the model: 0.9403931515535827\n",
      "\n",
      "Classification report: \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.95      0.93      0.94       789\n",
      "           1       0.93      0.95      0.94       788\n",
      "\n",
      "    accuracy                           0.94      1577\n",
      "   macro avg       0.94      0.94      0.94      1577\n",
      "weighted avg       0.94      0.94      0.94      1577\n",
      "\n",
      "\n",
      "Confusion matrix: \n",
      "[[733  56]\n",
      " [ 38 750]]\n",
      "\n",
      "------------------------------------------------------\n",
      "Model Saved\n",
      "RandomForestClassifier()\n",
      "Accuracy of the model: 0.8922003804692454\n",
      "\n",
      "Classification report: \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.89      0.90      0.89       789\n",
      "           1       0.90      0.89      0.89       788\n",
      "\n",
      "    accuracy                           0.89      1577\n",
      "   macro avg       0.89      0.89      0.89      1577\n",
      "weighted avg       0.89      0.89      0.89      1577\n",
      "\n",
      "\n",
      "Confusion matrix: \n",
      "[[708  81]\n",
      " [ 89 699]]\n",
      "\n",
      "------------------------------------------------------\n",
      "Model Saved\n",
      "XGBClassifier(base_score=0.5, booster='gbtree', callbacks=None,\n",
      "              colsample_bylevel=1, colsample_bynode=1, colsample_bytree=1,\n",
      "              early_stopping_rounds=None, enable_categorical=False,\n",
      "              eval_metric=None, gamma=0, gpu_id=-1, grow_policy='depthwise',\n",
      "              importance_type=None, interaction_constraints='',\n",
      "              learning_rate=0.300000012, max_bin=256, max_cat_to_onehot=4,\n",
      "              max_delta_step=0, max_depth=6, max_leaves=0, min_child_weight=1,\n",
      "              missing=nan, monotone_constraints='()', n_estimators=100,\n",
      "              n_jobs=0, num_parallel_tree=1, predictor='auto', random_state=0,\n",
      "              reg_alpha=0, reg_lambda=1, ...)\n",
      "Accuracy of the model: 0.9308814204185162\n",
      "\n",
      "Classification report: \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.93      0.93      0.93       789\n",
      "           1       0.93      0.93      0.93       788\n",
      "\n",
      "    accuracy                           0.93      1577\n",
      "   macro avg       0.93      0.93      0.93      1577\n",
      "weighted avg       0.93      0.93      0.93      1577\n",
      "\n",
      "\n",
      "Confusion matrix: \n",
      "[[732  57]\n",
      " [ 52 736]]\n",
      "\n",
      "------------------------------------------------------\n",
      "Model Saved\n",
      "LGBMClassifier()\n",
      "Accuracy of the model: 0.9334178820545339\n",
      "\n",
      "Classification report: \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.94      0.93      0.93       789\n",
      "           1       0.93      0.94      0.93       788\n",
      "\n",
      "    accuracy                           0.93      1577\n",
      "   macro avg       0.93      0.93      0.93      1577\n",
      "weighted avg       0.93      0.93      0.93      1577\n",
      "\n",
      "\n",
      "Confusion matrix: \n",
      "[[735  54]\n",
      " [ 51 737]]\n",
      "\n",
      "------------------------------------------------------\n",
      "CPU times: total: 5min 2s\n",
      "Wall time: 3min 10s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "np.random.seed(2022) \n",
    "for classifier in classifiers:\n",
    "    \n",
    "    # Pipeline\n",
    "    model = Pipeline(steps=[\n",
    "        (\"preprocessor\", TextPreprocessor()),\n",
    "        (\"vectorizer\", TfidfVectorizer()),\n",
    "        (\"clf\", classifier)]) \n",
    "        \n",
    "    # Fit And Predict Model\n",
    "    model = model.fit(xtrain, ytrain)\n",
    "    ypred = model.predict(xtest)\n",
    "    pickle.dump(model, open('../models/model_selection/' + classifier.__class__.__name__ + '.pkl', 'wb'))\n",
    "    print('Model Saved')\n",
    "    # Print Metrics\n",
    "    print(classifier)\n",
    "    evaluate_model(ytest, ypred)\n",
    "    print('------------------------------------------------------')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Selection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Choosing a model is not a trivial choice and it will mostly depend on the particular problem. In a classification problem accuracy on the test set is the most popular metric to assess the performance of a model.  \n",
    "In our case, we have a balanced dataset, we don't have any particular requirements,  of cost-sensitivity, time, or computational resources and we do not have to deploy our model. Therefore I will use accuracy to evaluate the best model.  \n",
    "Nevertheless, we can imagine a scenario in which we were developing a real tool or app to detect fake news. In that case, we might want to detect as many fake news as possible even if we classify some real news as fake or we might have a hard time creating a balanced dataset due to most of the news are real. In such cases, metrics like accuracy would not be very useful.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Winners\n",
    "Winners: **SGDClassifier and LightGBM**  \n",
    "Special Mention: Logistic Regression, Passive Aggressive Classifier  \n",
    "In the following notebook we will optimize the hyperparameters of the winners. "
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "115e7b4be8b202d8681536be4587d690d40232cbe799889ab736e9ee853895af"
  },
  "kernelspec": {
   "display_name": "Python 3.8.12 ('core_models')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
